{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df58427b",
   "metadata": {},
   "source": [
    "# Reconstructing Artistic Representations Using Masked Auto-Encoders (MAEs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc50e6",
   "metadata": {},
   "source": [
    "This project aims to re-implement a Masked Autoencoder (MAE), a self-supervised learning framework introduced by He et al. (2021). MAEs learn meaningful visual representations by reconstructing images from partially masked inputs, forcing the model to infer missing content from context. While MAEs are shown scalable on natural images, their applicability to artistic paintings, where brushstrokes, color palettes, and composition carry artistic intent, remains unexplored. Unlike photographs, paintings are constructed rather than captured, raising the question: Can MAEs learn meaningful representations of artistic intent? By ablating masking strategies (random vs. stroke-aware masking) and measuring image reconstruction, we evaluate whether MAEs are applicable to the art domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42399c2e",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f4711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from einops import rearrange\n",
    "from timm.models.vision_transformer import Block\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from utils import WikiArtDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e053234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e049f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"data\\WikiArtDataset.zip\"\n",
    "DATASET_PREVIEWS_PATH = \"data\\WikiArtDataset_Previews\"\n",
    "\n",
    "MODEL_1_PATH = \"models/mae_1_epoch_11.pt\"\n",
    "MODEL_2_PATH = \"models/mae_2_epoch_13.pt\"\n",
    "MODEL_3_PATH = \"models/mae_3_epoch_9.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb972d",
   "metadata": {},
   "source": [
    "# Wiki Art Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592ac2f",
   "metadata": {},
   "source": [
    "We will be employing the [Wiki-Art : Visual Art Encyclopedia](https://www.kaggle.com/datasets/ipythonx/wikiart-gangogh-creating-art-gan?resource=download) sourced by Innat on Kaggle. This imageset is composed of artworks scraped from [WikiArt.org](WikiArt.org), providing various types of art styles,\n",
    "\n",
    "- `abstract`: 14999 counts\n",
    "- `animal-painting`: 1798\n",
    "- `cityscape`: 6598\n",
    "- `figurative`: 4500\n",
    "- `flower-painting`: 1800\n",
    "- `genre-painting`: 14997\n",
    "- `landscape`: 15000\n",
    "- `marina`: 1800\n",
    "- `mythological-painting`: 2099\n",
    "- `nude-painting-nu`: 3000\n",
    "- `portrait`: 14999\n",
    "- `religious-painting`: 8400\n",
    "- `still-life`: 2996\n",
    "- `symbolic-painting`: 299\n",
    "\n",
    "The MAE model will be trained, validated, and tested on a combined and shuffled dataset of these images. Hopefully, we can expect consistent results across all the art contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview examples\n",
    "preview_images = os.listdir(DATASET_PREVIEWS_PATH)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (ax, image_file) in enumerate(zip(axes, preview_images)):\n",
    "    image_path = os.path.join(DATASET_PREVIEWS_PATH, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    image_art_type = image_file.split(\".\")[0]\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f\"{image_art_type}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "# Turn off any remaining empty subplots\n",
    "for j in range(len(preview_images), 3*5):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VALID_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "\n",
    "transforms = T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomGrayscale(p=0.1),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wiki_art_dataset = WikiArtDataset(DATASET_PATH, transform=transforms)\n",
    "\n",
    "# Split training set\n",
    "train_size = int(0.8 * len(full_wiki_art_dataset))\n",
    "temp_size = len(full_wiki_art_dataset) - train_size\n",
    "train_dataset, temp_dataset = random_split(\n",
    "    full_wiki_art_dataset, \n",
    "    [train_size, temp_size]\n",
    ")\n",
    "\n",
    "# Split validation and test set\n",
    "val_size = int(0.5 * len(temp_dataset))\n",
    "test_size = len(temp_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(\n",
    "    temp_dataset, \n",
    "    [val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594b48a",
   "metadata": {},
   "source": [
    "# Final MAE Implementation\n",
    "\n",
    "This is an outlined workflow of the training and testing process of the MAE,\n",
    "\n",
    "1. Masking:\n",
    "    - For each input image, divide them into patches and randomly select 75% of the patches to mask\n",
    "2. Encoding:\n",
    "    - Pass only the visible patches through the encoder\n",
    "    - The output should be the latent representations for each unmasked patch\n",
    "3. Decoding:\n",
    "    - The decoder takes in the encoded masked patches (output of the encoder) and Mask tokens (learnable placeholders for masked patches)\n",
    "    - Goal is to reconstruct all patches of the input image, including visible patches\n",
    "\n",
    "![architecture](imgs/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a883b9",
   "metadata": {},
   "source": [
    "### Input Masking\n",
    "\n",
    "In a MAE, input images are instead masked or \"patched out\" from the model during training. Masking is performed by splitting the images into fixed sized patches and randomly selecting 75% of the patches to mask. The motivation of this to not only reconstruct the missing parts of a masked image but to also force the model to learn meaningful representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9058f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMasker(nn.Module):\n",
    "    def __init__(self, mask_ratio=0.75, patch_size=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Split into patches\n",
    "        x_patches = rearrange(\n",
    "            x,\n",
    "            'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
    "            p1=self.patch_size,\n",
    "            p2=self.patch_size\n",
    "        )\n",
    "        num_patches = x_patches.shape[1]\n",
    "\n",
    "        # Generate random mask\n",
    "        noise = torch.rand(B, num_patches, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "\n",
    "        # Get masked/unmasked indices\n",
    "        num_masked = int(num_patches * self.mask_ratio)\n",
    "        mask_indices = ids_shuffle[:, :num_masked]\n",
    "        unmasked_indices = ids_shuffle[:, num_masked:]\n",
    "\n",
    "        return x_patches, mask_indices, unmasked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate patch masker\n",
    "test_image = Image.open(\"data\\WikiArtDataset_Previews\\symbolic-painting.jpg\").convert('RGB')\n",
    "test_image = transforms(image).unsqueeze(0)\n",
    "\n",
    "test_mask_ratio = 0.25\n",
    "test_patch_size = 16\n",
    "\n",
    "masker = PatchMasker(mask_ratio=test_mask_ratio, patch_size=test_patch_size)\n",
    "patches, mask_indices, unmasked_indices = masker(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization (replace masked patches with gray)\n",
    "masked_patches = patches.clone()\n",
    "masked_patches[0, mask_indices[0]] = 0.5\n",
    "\n",
    "# Reconstruct image from patches\n",
    "B, C, H, W = test_image.shape\n",
    "masked_image = rearrange(\n",
    "    masked_patches,\n",
    "    'b (h w) (p1 p2 c) -> b c (h p1) (w p2)',\n",
    "    h=H//16,\n",
    "    w=W//16,\n",
    "    p1=16,\n",
    "    p2=16,\n",
    "    c=C\n",
    ")\n",
    "\n",
    "# Visualize side by side\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0].permute(1, 2, 0))\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(masked_image[0].permute(1, 2, 0))\n",
    "plt.title(f\"Masked ({100*test_mask_ratio}% of {test_patch_size}x{test_patch_size} patches)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914e99c",
   "metadata": {},
   "source": [
    "### Final MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1518ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Encoder(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, in_channels=3, embed_dim=768, num_layers=12, num_heads=12):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Linear(patch_size**2 * in_channels, embed_dim)\n",
    "\n",
    "        # Positional embeddings and CLS token\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim, dtype=torch.float32))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim, dtype=torch.float32))\n",
    "\n",
    "        # Transformer\n",
    "        self.blocks = nn.Sequential(*[Block(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x_patches, unmasked_indices):\n",
    "        B = x_patches.shape[0]\n",
    "\n",
    "        # Extract unmasked patches\n",
    "        x_unmasked = torch.stack([\n",
    "            x_patches[b, unmasked_indices[b]] for b in range(B)\n",
    "        ], dim=0)\n",
    "\n",
    "        x_unmasked = self.patch_embed(x_unmasked)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        pos_embed = self.pos_embed[:, 1:].expand(B, -1, -1)\n",
    "        pos_embed_unmasked = torch.stack([\n",
    "            pos_embed[b, unmasked_indices[b]] for b in range(B)\n",
    "        ], dim=0)\n",
    "        x_unmasked = x_unmasked + pos_embed_unmasked.to(x_unmasked.dtype)\n",
    "\n",
    "        # Append cls token\n",
    "        cls_token = self.cls_token.expand(B, -1, -1).to(x_unmasked.dtype)\n",
    "        x_unmasked = torch.cat([cls_token, x_unmasked], dim=1)\n",
    "\n",
    "        # Transformer\n",
    "        x_encoded = self.blocks(x_unmasked)\n",
    "        return self.norm(x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Decoder(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, in_channels=3, encoder_embed_dim=768, decoder_embed_dim=512, num_layers=4, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Project encoder output to decoder dimension\n",
    "        self.decoder_embed = nn.Linear(encoder_embed_dim, decoder_embed_dim)\n",
    "\n",
    "        # Positional embeddings and mask token\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_embed_dim, dtype=torch.float32))\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim, dtype=torch.float32))\n",
    "\n",
    "        # Decoder transformer\n",
    "        self.blocks = nn.Sequential(*[Block(decoder_embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_channels)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        nn.init.trunc_normal_(self.mask_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x_encoded, mask_indices, unmasked_indices):\n",
    "        B = x_encoded.shape[0]\n",
    "\n",
    "        # Project encoder output\n",
    "        x_decoder = self.decoder_embed(x_encoded)\n",
    "\n",
    "        # Prepare full sequence: CLS + unmasked + masked\n",
    "        x_full = torch.zeros(\n",
    "            B,\n",
    "            self.num_patches + 1,\n",
    "            x_decoder.shape[-1],\n",
    "            dtype=x_decoder.dtype,\n",
    "            device=x_decoder.device\n",
    "        )\n",
    "\n",
    "        # Set CLS token\n",
    "        x_full[:, 0] = x_decoder[:, 0]\n",
    "\n",
    "        # Set unmasked patch tokens\n",
    "        for b in range(B):\n",
    "            x_full[b, unmasked_indices[b] + 1] = x_decoder[b, 1:]\n",
    "\n",
    "        # Insert mask tokens\n",
    "        mask_tokens = self.mask_token.expand(B, mask_indices.shape[1], -1).to(x_decoder.dtype)\n",
    "        for b in range(B):\n",
    "            x_full[b, mask_indices[b] + 1] = mask_tokens[b]\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x_full = x_full + self.decoder_pos_embed[:, :x_full.size(1)].to(x_full.dtype)\n",
    "\n",
    "        # Transformer and prediction\n",
    "        x_decoded = self.blocks(x_full)\n",
    "        pred_patches = self.decoder_pred(x_decoded[:, 1:])\n",
    "        \n",
    "        return pred_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=224, \n",
    "        patch_size=16, \n",
    "        in_channels=3,\n",
    "        encoder_embed_dim=256,\n",
    "        decoder_embed_dim=128,\n",
    "        encoder_depth=4,\n",
    "        decoder_depth=1,\n",
    "        encoder_heads=4,\n",
    "        decoder_heads=2,\n",
    "        mask_ratio=0.75,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Modules\n",
    "        self.patch_masker = PatchMasker(mask_ratio, patch_size)\n",
    "        self.encoder = MAE_Encoder(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=encoder_embed_dim,\n",
    "            num_layers=encoder_depth,\n",
    "            num_heads=encoder_heads,\n",
    "        )\n",
    "        self.decoder = MAE_Decoder(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            encoder_embed_dim=encoder_embed_dim,\n",
    "            decoder_embed_dim=decoder_embed_dim,\n",
    "            num_layers=decoder_depth,\n",
    "            num_heads=decoder_heads,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_patches, mask_indices, unmasked_indices = self.patch_masker(x)\n",
    "        x_encoded = self.encoder(x_patches, unmasked_indices)\n",
    "        pred_patches = self.decoder(x_encoded, mask_indices, unmasked_indices)\n",
    "\n",
    "        pred_img = rearrange(\n",
    "            pred_patches,\n",
    "            'b (h w) (p1 p2 c) -> b c (h p1) (w p2)',\n",
    "            h=int((x.shape[2] / self.patch_size)),\n",
    "            w=int((x.shape[3] / self.patch_size)),\n",
    "            p1=self.patch_size,\n",
    "            p2=self.patch_size,\n",
    "            c=x.shape[1],\n",
    "        )\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        num_patches = (H // self.patch_size) * (W // self.patch_size)\n",
    "        \n",
    "        mask = torch.ones(B, num_patches, device=x.device)\n",
    "        mask.scatter_(1, unmasked_indices, 0)\n",
    "        \n",
    "        # Reshape to spatial dimensions\n",
    "        mask = mask.reshape(B, 1, H // self.patch_size, W // self.patch_size)\n",
    "        mask = F.interpolate(mask.float(), size=(H, W), mode='nearest')\n",
    "\n",
    "        return pred_img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = MAE(\n",
    "    encoder_embed_dim=384,\n",
    "    decoder_embed_dim=256,\n",
    "    encoder_depth=6,\n",
    "    decoder_depth=2,\n",
    "    encoder_heads=6,\n",
    "    decoder_heads=4,\n",
    ").cuda()\n",
    "\n",
    "checkpoint = torch.load(MODEL_1_PATH)\n",
    "best_model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f505eb",
   "metadata": {},
   "source": [
    "### Evaluate Final MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_MAE(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_pixels = 0\n",
    "\n",
    "    criterion = torch.nn.MSELoss(reduction='sum').cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "        \n",
    "        for images in progress_bar:\n",
    "            images = images[0].cuda(non_blocking=True)\n",
    "\n",
    "            predicted_img, mask = model(images)\n",
    "            loss = criterion(predicted_img * mask, images * mask)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_pixels += mask.sum().item()\n",
    "            \n",
    "            # Update progress bar description with current batch stats\n",
    "            progress_bar.set_postfix({\n",
    "                'batch_loss': loss.item() / mask.sum().item(),\n",
    "                'avg_loss': total_loss / total_pixels if total_pixels > 0 else 0\n",
    "            })\n",
    "\n",
    "    avg_loss = total_loss / total_pixels if total_pixels > 0 else 0\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcf807",
   "metadata": {},
   "outputs": [],
   "source": [
    "_cached_reconstruction_images = None\n",
    "\n",
    "def show_reconstructions(model, data_loader, num_images=3):\n",
    "    global _cached_reconstruction_images\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if _cached_reconstruction_images is None:\n",
    "        images, _ = next(iter(data_loader))\n",
    "        _cached_reconstruction_images = images[:num_images]\n",
    "    \n",
    "    images = _cached_reconstruction_images.cuda(non_blocking=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reconstructions, _ = model(images)\n",
    "    \n",
    "    # Convert to numpy and properly format\n",
    "    original_imgs = images.cpu().numpy()\n",
    "    recon_imgs = reconstructions.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(num_images):\n",
    "        # Original\n",
    "        plt.subplot(2, num_images, i+1)\n",
    "        plt.imshow(np.transpose(original_imgs[i], (1, 2, 0)))\n",
    "        plt.title(f\"Original {i+1}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Reconstruction\n",
    "        plt.subplot(2, num_images, i+num_images+1)\n",
    "        plt.imshow(np.transpose(recon_imgs[i], (1, 2, 0)))\n",
    "        plt.title(f\"Reconstructed {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = evaluate_MAE(best_model, val_loader)\n",
    "\n",
    "print(f\"Validation Loss (MSE on masked regions): {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b0cc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_reconstructions(best_model, val_loader, num_images=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbdabf",
   "metadata": {},
   "source": [
    "# Other Experimental MAE Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304b843",
   "metadata": {},
   "source": [
    "### MAE Model 2 (Implemented in `MAE_model_2.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52efd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.MAE_model_2 import MAE as MAE_MODEL_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f47e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model 2\n",
    "model_2 = MAE_MODEL_2().cuda()\n",
    "\n",
    "checkpoint = torch.load(MODEL_2_PATH)\n",
    "model_2.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fef665",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = evaluate_MAE(model_2, val_loader)\n",
    "\n",
    "print(f\"Validation Loss (MSE on masked regions): {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(model_2, val_loader, num_images=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffcf9bb",
   "metadata": {},
   "source": [
    "### MAE Model 3 (Implemented in `MAE_model_3.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2514555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model 3\n",
    "model_3 = MAE(\n",
    "    encoder_embed_dim=192,\n",
    "    decoder_embed_dim=512,\n",
    "    encoder_depth=12,\n",
    "    decoder_depth=8,\n",
    "    encoder_heads=3,\n",
    "    decoder_heads=16,\n",
    ").cuda()\n",
    "\n",
    "checkpoint = torch.load(MODEL_3_PATH)\n",
    "model_3.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = evaluate_MAE(model_3, val_loader)\n",
    "\n",
    "print(f\"Validation Loss (MSE on masked regions): {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructions(model_3, val_loader, num_images=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f7964",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i in range(1, 4):\n",
    "    loss_df = pd.read_csv(f'data/model_loss/mae_model_{str(i)}.csv')\n",
    "    \n",
    "    ax = axes[i - 1]\n",
    "    ax.plot(loss_df['epoch'], loss_df['loss'])\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"MSE\")\n",
    "    ax.set_xticks(loss_df['epoch'])\n",
    "    ax.set_title(f\"MAE Model {i} Loss Curve\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec27a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_practice",
   "language": "python",
   "name": "pytorch_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
