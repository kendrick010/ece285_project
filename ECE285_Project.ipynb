{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df58427b",
   "metadata": {},
   "source": [
    "# Reconstructing Artistic Representations Using Masked Auto-Encoders (MAEs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc50e6",
   "metadata": {},
   "source": [
    "This project aims to re-implement a Masked Autoencoder (MAE), a self-supervised learning framework introduced by He et al. (2021). MAEs learn meaningful visual representations by reconstructing images from partially masked inputs, forcing the model to infer missing content from context. While MAEs are shown scalable on natural images, their applicability to artistic paintings, where brushstrokes, color palettes, and composition carry artistic intent, remains unexplored. Unlike photographs, paintings are constructed rather than captured, raising the question: Can MAEs learn meaningful representations of artistic intent? By ablating masking strategies (random vs. stroke-aware masking) and measuring image reconstruction, we evaluate whether MAEs are applicable to the art domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42399c2e",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f4711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from einops import rearrange\n",
    "from timm.models.vision_transformer import Block\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "from utils import WikiArtDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e053234",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e049f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"data\\WikiArtDataset.zip\"\n",
    "DATASET_PREVIEWS_PATH = \"data\\WikiArtDataset_Previews\"\n",
    "TRAIN = False\n",
    "\n",
    "MODEL_DIR = \"models\"\n",
    "LOAD_MODEL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb972d",
   "metadata": {},
   "source": [
    "# Wiki Art Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592ac2f",
   "metadata": {},
   "source": [
    "We will be employing the [Wiki-Art : Visual Art Encyclopedia](https://www.kaggle.com/datasets/ipythonx/wikiart-gangogh-creating-art-gan?resource=download) sourced by Innat on Kaggle. This imageset is composed of artworks scraped from [WikiArt.org](WikiArt.org), providing various types of art styles,\n",
    "\n",
    "- `abstract`: 14999 counts\n",
    "- `animal-painting`: 1798\n",
    "- `cityscape`: 6598\n",
    "- `figurative`: 4500\n",
    "- `flower-painting`: 1800\n",
    "- `genre-painting`: 14997\n",
    "- `landscape`: 15000\n",
    "- `marina`: 1800\n",
    "- `mythological-painting`: 2099\n",
    "- `nude-painting-nu`: 3000\n",
    "- `portrait`: 14999\n",
    "- `religious-painting`: 8400\n",
    "- `still-life`: 2996\n",
    "- `symbolic-painting`: 299\n",
    "\n",
    "The MAE model will be trained, validated, and tested on a combined and shuffled dataset of these images. Hopefully, we can expect consistent results across all the art contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2409c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview examples\n",
    "preview_images = os.listdir(DATASET_PREVIEWS_PATH)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (ax, image_file) in enumerate(zip(axes, preview_images)):\n",
    "    image_path = os.path.join(DATASET_PREVIEWS_PATH, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    image_art_type = image_file.split(\".\")[0]\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    ax.set_title(f\"{image_art_type}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "# Turn off any remaining empty subplots\n",
    "for j in range(len(preview_images), 3*5):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f8ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "TRAIN_SPLIT = 0.7\n",
    "VALID_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "\n",
    "transforms = T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomGrayscale(p=0.1),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b8f6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wiki_art_dataset = WikiArtDataset(DATASET_PATH, transform=transforms)\n",
    "\n",
    "# Split training set\n",
    "train_size = int(0.8 * len(full_wiki_art_dataset))\n",
    "temp_size = len(full_wiki_art_dataset) - train_size\n",
    "train_dataset, temp_dataset = random_split(\n",
    "    full_wiki_art_dataset, \n",
    "    [train_size, temp_size]\n",
    ")\n",
    "\n",
    "# Split validation and test set\n",
    "val_size = int(0.5 * len(temp_dataset))\n",
    "test_size = len(temp_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(\n",
    "    temp_dataset, \n",
    "    [val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594b48a",
   "metadata": {},
   "source": [
    "# Base MAE Implementation\n",
    "\n",
    "This is an outlined workflow of the training and testing process of the MAE,\n",
    "\n",
    "1. Masking:\n",
    "    - For each input image, divide them into patches and randomly select 75% of the patches to mask\n",
    "2. Encoding:\n",
    "    - Pass only the visible patches through the encoder\n",
    "    - The output should be the latent representations for each unmasked patch\n",
    "3. Decoding:\n",
    "    - The decoder takes in the encoded masked patches (output of the encoder) and Mask tokens (learnable placeholders for masked patches)\n",
    "    - Goal is to reconstruct all patches of the input image, including visible patches\n",
    "\n",
    "![architecture](imgs/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a883b9",
   "metadata": {},
   "source": [
    "### Input Masking\n",
    "\n",
    "In a MAE, input images are instead masked or \"patched out\" from the model during training. Masking is performed by splitting the images into fixed sized patches and randomly selecting 75% of the patches to mask. The motivation of this to not only reconstruct the missing parts of a masked image but to also force the model to learn meaningful representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9058f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMasker(nn.Module):\n",
    "    def __init__(self, mask_ratio=0.75, patch_size=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Split into patches\n",
    "        x_patches = rearrange(\n",
    "            x,\n",
    "            'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
    "            p1=self.patch_size,\n",
    "            p2=self.patch_size\n",
    "        )\n",
    "        num_patches = x_patches.shape[1]\n",
    "\n",
    "        # Generate random mask\n",
    "        noise = torch.rand(B, num_patches, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "\n",
    "        # Get masked/unmasked indices\n",
    "        num_masked = int(num_patches * self.mask_ratio)\n",
    "        mask_indices = ids_shuffle[:, :num_masked]\n",
    "        unmasked_indices = ids_shuffle[:, num_masked:]\n",
    "\n",
    "        return x_patches, mask_indices, unmasked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e88f6389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate patch masker\n",
    "test_batch = next(iter(train_loader))\n",
    "test_image = test_batch[0][0].unsqueeze(0)\n",
    "test_mask_ratio = 0.25\n",
    "test_patch_size = 16\n",
    "\n",
    "masker = PatchMasker(mask_ratio=test_mask_ratio, patch_size=test_patch_size)\n",
    "patches, mask_indices, unmasked_indices = masker(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0604a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization (replace masked patches with gray)\n",
    "masked_patches = patches.clone()\n",
    "masked_patches[0, mask_indices[0]] = 0.5\n",
    "\n",
    "# Reconstruct image from patches\n",
    "B, C, H, W = test_image.shape\n",
    "masked_image = rearrange(\n",
    "    masked_patches,\n",
    "    'b (h w) (p1 p2 c) -> b c (h p1) (w p2)',\n",
    "    h=H//16,\n",
    "    w=W//16,\n",
    "    p1=16,\n",
    "    p2=16,\n",
    "    c=C\n",
    ")\n",
    "\n",
    "# Visualize side by side\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_image[0].permute(1, 2, 0))\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(masked_image[0].permute(1, 2, 0))\n",
    "plt.title(f\"Masked ({100*test_mask_ratio}% of {test_patch_size}x{test_patch_size} patches)\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914e99c",
   "metadata": {},
   "source": [
    "### Base MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b1518ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Encoder(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, in_channels=3, embed_dim=768, num_layers=12, num_heads=12):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = nn.Linear(patch_size**2 * in_channels, embed_dim)\n",
    "\n",
    "        # Positional embeddings and CLS token\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim, dtype=torch.float32))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim, dtype=torch.float32))\n",
    "\n",
    "        # Transformer\n",
    "        self.blocks = nn.Sequential(*[Block(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x_patches, unmasked_indices):\n",
    "        B = x_patches.shape[0]\n",
    "\n",
    "        # Extract unmasked patches\n",
    "        x_unmasked = torch.stack([\n",
    "            x_patches[b, unmasked_indices[b]] for b in range(B)\n",
    "        ], dim=0)\n",
    "\n",
    "        x_unmasked = self.patch_embed(x_unmasked)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        pos_embed = self.pos_embed[:, 1:].expand(B, -1, -1)\n",
    "        pos_embed_unmasked = torch.stack([\n",
    "            pos_embed[b, unmasked_indices[b]] for b in range(B)\n",
    "        ], dim=0)\n",
    "        x_unmasked = x_unmasked + pos_embed_unmasked.to(x_unmasked.dtype)\n",
    "\n",
    "        # Append cls token\n",
    "        cls_token = self.cls_token.expand(B, -1, -1).to(x_unmasked.dtype)\n",
    "        x_unmasked = torch.cat([cls_token, x_unmasked], dim=1)\n",
    "\n",
    "        # Transformer\n",
    "        x_encoded = self.blocks(x_unmasked)\n",
    "        return self.norm(x_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2144367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE_Decoder(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, in_channels=3, encoder_embed_dim=768, decoder_embed_dim=512, num_layers=4, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Project encoder output to decoder dimension\n",
    "        self.decoder_embed = nn.Linear(encoder_embed_dim, decoder_embed_dim)\n",
    "\n",
    "        # Positional embeddings and mask token\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, decoder_embed_dim, dtype=torch.float32))\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim, dtype=torch.float32))\n",
    "\n",
    "        # Decoder transformer\n",
    "        self.blocks = nn.Sequential(*[Block(decoder_embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_channels)\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        nn.init.trunc_normal_(self.mask_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x_encoded, mask_indices, unmasked_indices):\n",
    "        B = x_encoded.shape[0]\n",
    "\n",
    "        # Project encoder output\n",
    "        x_decoder = self.decoder_embed(x_encoded)\n",
    "\n",
    "        # Prepare full sequence: CLS + unmasked + masked\n",
    "        x_full = torch.zeros(\n",
    "            B,\n",
    "            self.num_patches + 1,\n",
    "            x_decoder.shape[-1],\n",
    "            dtype=x_decoder.dtype,\n",
    "            device=x_decoder.device\n",
    "        )\n",
    "\n",
    "        # Set CLS token\n",
    "        x_full[:, 0] = x_decoder[:, 0]\n",
    "\n",
    "        # Set unmasked patch tokens\n",
    "        for b in range(B):\n",
    "            x_full[b, unmasked_indices[b] + 1] = x_decoder[b, 1:]\n",
    "\n",
    "        # Insert mask tokens\n",
    "        mask_tokens = self.mask_token.expand(B, mask_indices.shape[1], -1).to(x_decoder.dtype)\n",
    "        for b in range(B):\n",
    "            x_full[b, mask_indices[b] + 1] = mask_tokens[b]\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x_full = x_full + self.decoder_pos_embed[:, :x_full.size(1)].to(x_full.dtype)\n",
    "\n",
    "        # Transformer and prediction\n",
    "        x_decoded = self.blocks(x_full)\n",
    "        pred_patches = self.decoder_pred(x_decoded[:, 1:])\n",
    "        return pred_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4a0be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=224, \n",
    "        patch_size=16, \n",
    "        in_channels=3,\n",
    "        encoder_embed_dim=256,\n",
    "        decoder_embed_dim=128,\n",
    "        encoder_depth=4,\n",
    "        decoder_depth=1,\n",
    "        encoder_heads=4,\n",
    "        decoder_heads=2,\n",
    "        mask_ratio=0.75,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Modules\n",
    "        self.patch_masker = PatchMasker(mask_ratio, patch_size)\n",
    "        self.encoder = MAE_Encoder(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embed_dim=encoder_embed_dim,\n",
    "            num_layers=encoder_depth,\n",
    "            num_heads=encoder_heads,\n",
    "        )\n",
    "        self.decoder = MAE_Decoder(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            encoder_embed_dim=encoder_embed_dim,\n",
    "            decoder_embed_dim=decoder_embed_dim,\n",
    "            num_layers=decoder_depth,\n",
    "            num_heads=decoder_heads,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Mask the input\n",
    "        x_patches, mask_indices, unmasked_indices = self.patch_masker(x)\n",
    "\n",
    "        # 2. Encode visible patches\n",
    "        x_encoded = self.encoder(x_patches, unmasked_indices)\n",
    "\n",
    "        # 3. Decode all patches\n",
    "        pred_patches = self.decoder(x_encoded, mask_indices, unmasked_indices)\n",
    "\n",
    "        # 4. Reconstruct image\n",
    "        pred_img = rearrange(\n",
    "            pred_patches,\n",
    "            'b (h w) (p1 p2 c) -> b c (h p1) (w p2)',\n",
    "            h=int((x.shape[2] / self.patch_size)),\n",
    "            w=int((x.shape[3] / self.patch_size)),\n",
    "            p1=self.patch_size,\n",
    "            p2=self.patch_size,\n",
    "            c=x.shape[1],\n",
    "        )\n",
    "\n",
    "        return pred_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3550f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mae(model, optimizer, criterion, scaler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx, (images, _) in enumerate(train_loader):\n",
    "            images = images.cuda()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(dtype=torch.float16):\n",
    "                reconstructed = model(images)\n",
    "                loss = criterion(reconstructed, images)\n",
    "\n",
    "            # Gradient scaling + backward\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0: print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "        # Save checkpoint every epoch\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'loss': total_loss / len(train_loader),\n",
    "        }\n",
    "        torch.save(\n",
    "            checkpoint, \n",
    "            os.path.join(MODEL_DIR, f\"mae_epoch_{epoch}.pt\")\n",
    "        )\n",
    "        print(f\"Epoch {epoch}: Loss = {total_loss / len(train_loader):.4f} | Checkpoint saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c74932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "lr = 1e-4\n",
    "\n",
    "model = MAE(\n",
    "    image_size=224, \n",
    "    patch_size=16, \n",
    "    in_channels=3,\n",
    "    encoder_embed_dim=384,\n",
    "    decoder_embed_dim=256,\n",
    "    encoder_depth=6,\n",
    "    decoder_depth=2,\n",
    "    encoder_heads=6,\n",
    "    decoder_heads=4,\n",
    "    mask_ratio=0.75,\n",
    ").cuda()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    checkpoint_model = \"models\\mae_epoch_1.pt\"\n",
    "    checkpoint = torch.load(checkpoint_model)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "\n",
    "train_mae(\n",
    "    model, \n",
    "    optimizer, \n",
    "    criterion, \n",
    "    scaler, \n",
    "    num_epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbdabf",
   "metadata": {},
   "source": [
    "# Final MAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e38d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_practice",
   "language": "python",
   "name": "pytorch_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
